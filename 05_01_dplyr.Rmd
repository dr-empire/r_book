# Datenaufbereitung

```{r, echo = FALSE, warning=FALSE, message=FALSE, error=FALSE}
pacman::p_load(tidyverse, sjlabelled)
# Einlesen der Daten
data <- expss::read_labelled_csv2("data/ZA6738_v1-0-0_generation_z_labelled.csv") %>% 
  mutate(alter = sjlabelled::remove_labels(alter, labels = 1:23)) %>% 
  select(lfdn, alter, geschlecht, bundesland, verbundenheit_dtl, verbundenheit_europa) %>% 
  mutate(verbundenheit_europa = ifelse(lfdn == 1680, 99, verbundenheit_europa))

data$verbundenheit_europa <- set_labels(data$verbundenheit_europa, labels = get_labels(data$verbundenheit_dtl))
```

Datenaufbereitung (*data wrangling*) bezeichnet den Prozess, in dem Rohdaten so verändert, sortiert, umstrukturiert und ausgewählt werden, dass man sie für die anvisierte Analyse verwenden kann. 

Im einzelnen werden in diesem Kapitel die folgenden Funktionen erklärt:

- `filter()` zur Auswahl von Fällen

- `arrange()` zur Sortierung von Fällen

- `rec()` zum Umcodieren von Variablen

- `mutate()` zum Anlegen und Berechnen neuer Variablen

- `select()` zur Auswahl von Variablen

- `summarize()` um Daten zu verdichten 

Die letzte Funktion entfaltet besondere Stärken im Zusammenhang mit `group_by()`. Dadurch kann man Auswertungen oder bestimmte Datentransformationen nach Gruppen aufteilen. 

Fast alle der hier vorgestellten Funktionen gehören zum Paket `dplyr` aus dem Tidyverse. Die einzige Ausnahme bildet `rec()` aus dem Paket `sjmisc`. Obwohl sie aus unterschiedlichen Paketen stammen folgen alle dem Tidyverse-Konzept und funktionieren auf ähnliche Weise [vgl. @Wickham_2017, Kap. 5.1.3]:

- Das erste Argument ist immer der Dataframe.

- Die folgenden Argumente beschreiben, wie der Dataframe umgeformt werden soll (ohne Anführungsstriche).

- Soll innerhalb der Funktionen auf Variablen aus dem Dataframe zugegriffen werden, kann man diese direkt ansprechen (also einfach nur `var_name` und **nicht** `data$var_name` oder `"var_name"`).

- Das Ergebnis ist immer ein Dataframe.


## Datensatz für dieses Kapitel

Als Datensatz dient in diesem Kapitel der "starwars"-Datensatz, der im Paket dplyr enthalten ist. Er enthält verschiedene Merkmale von Starwars-Figuren:

```{r echo=FALSE}
starwars
```


## Die Pipe {#pipe}

<img src="images/hex_pipe.png" alt="Hexsticker pipe"  style="float:right;margin-top:15;margin-left:15px;margin-bottom:5px">Bevor es mit den einzelnen Schritten der Datenaufbereitung losgeht, wird an dieser Stelle noch ein neuer Operator eingeführt, die *Pipe*. In R geschrieben durch die Zeichenfolge `%>%`. Eine Pipe kann man auch durch den Shortcut `Ctrl/Strg + Shift + m` einfügen. Merken Sie sich diesen Shortcut gut. Sie werden ihn oft brauchen!

Die Pipe macht etwas, das für Sie zunächst tendenziell unsinnig klingen muss: Sie leitet das Ergebnis einer Funktion als Argument an die nächste Funktion weiter. Gerade bei der Datenaufbereitung ist das jedoch sehr praktisch, weil man häufig mehrere Funktionen hintereinanderschalten muss: Man möcht z.B. zunächst ein paar Fälle herausfiltern, dann eine neue Variable bilden, alte Variablen löschen, andere Variablen umcodieren, dann Variablen auswählen, den Datensatz neu sortieren und schließlich nochmal ein paar Fälle herausfilten und zum Schluss eine Analyse machen. Zusammengefasst: Es sollen sehr viele Transformationen eines Datensatzes hintereinander geschaltet werden. 


### Der Aufbau im Detail
Hier der schematische Aufbau einer Datentransformation mit Pipe, damit Sie nachvollziehen können, wie der Pipe-Operator funktioniert:
```{r echo=FALSE}
transformation_1 <- function(a, b){a}
transformation_2 <- function(a, b){a}
transformation_3 <- function(a, b){a}
```

```{r}
new_data <- data %>% 
  transformation_1("do something") %>%
  transformation_2("do something else") %>%
  transformation_3("do something else else") 
```
Schauen wir uns mal zeilenweise an, was hier passiert:

1. Erste Zeile: Der Start

    - Zunächst wird ein neues Objekt `new_data` erzeugt, indem das alte Objekt `data` - also unser Datensatz - kopiert wird. Dieser Schritt ist immer dann nötig, wenn man mit dem Datensatz weiterarbeiten möchte. 

    - Nachdem die Operation durchgeführt wurde, wird das Ergebnis dieser Operation (also das neue Objekt `new_data`) mit der Pipe `%>%` an die nächste Zeile übergeben.

2. Zweite Zeile: Wo landet das Objekt `new_data`? Ich habe eben geschrieben, dass das Objekt an die nächste Zeile übergeben wurde. Es ist vielleicht etwas irritierend, das es gar nicht mehr zu sehen ist. Also wo ist es?

    - Es steckt in der Funktion dieser Zeile, also im `transformation_1()` und zwar als erstes Argument. Durch die Pipe ist es quasi unsichtbar. Gedanklich kann man sich den Befehl in dieser Zeile so vorstellen: `transformation_1(new_data, "do something")` - nur, dass man `new_data` dort nicht extra erwähnen muss, weil durch die Pipe in der vorhergehenden Zeile klar ist, dass dieses Objet das erste Argument ist.
  
    - Die Funktion `transformation_1` wird also mit den beiden Argumenten `new_data` und `"do something"` ausgeführt. Der Datensatz verändert sich entsprechend. Er behält aber den gleichen Namen.
  
    - Am Ende der Zeile steht wieder eine Pipe `%>%`. Auch sie leitet das Ergebnis der vorhergehenden Transformation an die nächste Zeile weiter.

3. Dritte Zeile: ...same procedere as every pipe...

    - Wieder landet der (nun einmal transformierte) Dataframe `new_data` als erstes Argument in einer Funktion, diesmal in `transformation_2()`.
  
    - Wieder wird der Dataframe irgendwie transformiert und heiß noch immer gleich.
  
    - Wieder wird er durch die Pipe am Ende der Zeile an die nächste Zeile übergeben.
  
4. Vierte Zeile: Das Ende naht.

    - Auch hier wieder das selbe Spiel wie zuvor: Der Datensatz landet als erstes Argument in der Funktion `transformation_3()`, die irgendwelche Operationen mit ihm durchführt.
  
    - Nach der Transformation ist allerdings Schluss, denn da ist keine weitere Pipe. Der nun dreifach transformierte Datensatz ist jetzt fertig und liegt als neues Objekt `new_data` vor. Sie finden es im Environment-Tab.


Insgesamt ist die Pipe-Schreibweise sehr übersichtlich, weil die einzelnen Transformationen schön untereinander aufgeführt werden. Man kann also sehr schnell erkennen, was mit dem Dataframe passiert.

Noch eine kleine Anmerkung zur ersten Zeile: Dort habe ich durch `new_data <- data` ein neues Objekt erzeugt. Das ist immer dann sinnvoll, wenn man nach der Transformation die Daten als Objekt vorliegen haben möchte, um damit z.B. verschiedene statistische Berechnungen durchzuführen. Manchmal benötigt man aber gar kein neues Objekt. Vielleicht möchte man nur temporär etwas ausgeben. In diesem Fall könnte man auch direkt mit `data %>%` starten. In diesem Kapitel werde ich beides benutzen, da es mir hier auch nicht immer darum geht, den Datensatz tatsächlich zu transformieren.

### Schlechtere Alternativen zur Pipe

Schauen wir uns einmal an, was die Alternativen zur Arbeit mit der Pipe wären. Es gibt 3:

- Selbstverständlich könnte man alle Datentransformationen nacheinander machen und dabei den Dataframe, den es zu bearbeiten gilt, immer wieder überschreiben. Das ist jedoch keine saubere Arbeitsweise, es ist sehr anfällig für Fehler. 

- Eine andere Option wäre es, jedes Mal ein neues Objekt zu erzeugen und die Objekte dann durchzunummerieren oder zu benennen (`data_1`, `data_2`, `data_3` oder `data_filtered`, `data_sorted`, `data_with_var_x`). Auch nicht sehr übersichtlich und ebenfalls fehleranfällig. 

- Die dritte Möglichkeit wäre es, Funktionen ineinander zu verschachteln, etwa so: `fun1(fun2(fun3(arg1, arg2)), arg1, arg2)`. R würde diese dann von innen nach außen abarbeiten. Das ist zwar sehr kompakt, allerdings ist es sehr schwer hier den Überblick zu behalten und auch hier sind Fehler (etwa bei der Klammersetzung) vorprogrammiert.

Besser sie gewöhnen sich die Arbeit mit der Pipe direkt an. Gerade für den Bereich Datenaufbereitung macht die Pipe sehr viel Sinn, weil in den Funktionen das Datenargument immer an der ersten Stelle steht. Das kommt der Pipe sehr entgegen, weil man den Dataframe so quasi von oben nach unten durch die Pipe leiten und in jedem Schritt ein bisschen weiter umformen kann. Auch wenn die Pipes in diesem Kapitel noch nicht besonders lang sein werden verwende ich diese Schreibweise, einfach, damit Sie sich daran gewöhnen.


## Filter: Fälle auswählen
Mit Filtern kann man die Fallzahl eines Datensatzes nach bestimmten Kriterien verringern, also Fälle herausfiltert, die man nicht benötigt bzw. momentan nicht berücksichtigen möchte.

- Fälle entfernen, die man grundsätzlich nicht im Datensatz haben wollte, z.B. Minderjährige, wenn man nur Erwachsene befragen wollte.

- Dubletten entfernen (falls aus Versehen ein Fall doppelt eingegeben wurde)

- Einen Datensatz für eine bestimmte Analyse erstellen, die sich nur auf eine Teilstichprobe bezieht: 

    - alle Folgen von Serien die länger als 60 Minuten sind 
    
    - nur nicht-männliche Befragte
    
    - alle Personen die YouTube oder Instagram regelmäßig nutzen 

<img src="images/ah_dplyr_filter.jpg" alt="Artwork by Allison Horst" style="width:1000px;margin-bottom:5px;margin-top:50px">
<div style="color:grey;text-align:right">Artwork by <a href="https://github.com/allisonhorst/stats-illustrations/">Allison Horst</a></div><br />

Im folgenden Beispiel möchte ich einen Datensatz erstellen, der nur Fälle von Personen enthält, die angegeben haben aus dem Bundesland Niedersachsen (Code 3) zu kommen. Bevor man einen Filter anwendet, sollte man sich aber zunächst einen Überblick über die Ausgangslage verschaffen. Ich lasse mir deshalb einmal die Anzahl der Zeilen im Datensatz ausgeben und schaue mir die ersten paar Fälle an:

```{r }
nrow(data)
head(data)
```

Okay, der ursprüngliche Datensatz hat 1006 Zeilen (Befragte) und bei den Bundesländern gibt es gemischte Werte (z.B. 1, 3, 6 usw.).

Als nächstes muss eine Filterbedingung festgelegt werden. Die Filterbedingung ist nach den Daten das zweite und zwingende Argument, dass die `filter()`-Funktion benötigt. Hier kommt der Datentyp "logical" ins Spiel, den wir [hier](#atomic_vector_types) besprochen haben. Anhand der Filterbedingung prüft die Funktion `filter()` für jeden Fall im Datensatz, ob eine zuvor von uns definierte Bedingung `TRUE` oder `FALSE` ist. Ist das Ergebnis der Prüfung `TRUE` verbleibt der Fall im Datensatz. Ist es `FALSE` wird der Fall aus dem Datensatz entfernt. Die Prüfung erfolgt anhand der relationalen Operatoren (z.B. `==` für "ist gleich", `!=` für "ist ungleich" oder `<` für "ist kleiner als").

Im Beispiel wollen wir alle Niedersachsen in einem Datensatz abspeichern. Wir müssen also die Bedingung "Das Bundesland ist gleich Niedersachsen" so formulieren, dass R sie versteht. Wir wissen bereits, dass Niedersachsen den Code 3 hat also ist die Filterbedingung: `bundesland == 3`. Man braucht hier zwingend doppelte Gleichzeichen. Dies ist nötig, weil das einfache Gleichzeichen von R als Zuweisungsoperator `<-` verstanden würde. Hier soll aber nichts zugewiesen sondern lediglich etwas verglichen werden.

```{r }
data_nds <- data %>% 
  filter(bundesland == 3)
```

Gar nicht so schwer, aber hat das auch funktioniert? Schauen wir uns nochmal die Fallzahl und den Datensatz genauer an:

```{r }
nrow(data_nds)
head(data_nds)
```

Tatsächlich! Im Datensatz sind jetzt nur noch n = 107 Fälle und in der Variable `bundesland` haben alle Fälle den Wert 3.

Jetzt machen wir es komplizierter. Wir möchten jetzt alle Personen haben die aus Niedersachsen oder Hamburg kommen und jünger als 18 Jahre sind. Um eine so komplexe Bedingung zu formulieren braucht man neben den relationalen Operatoren auch noch logische Operatoren und Klammer-Regeln.

Mit logischen Operatoren kann man Bedingungen verknüpfen oder gegenseitig ausschließen. Die Wichtigsten sind: 

- `&` für “und”

- `|` für “oder”

- `!` für “nicht”

Die Bedingung "aus Niedersachsen oder Hamburg und jünger als 18 Jahre" lässt sich also wie folgt formulieren: `(bundesland = 3 | bundesland = 2) & alter < 18`. Hier kommt es haargeanau auf die Klammern an. Wären sie nicht gesetzt würde R möglicherweise alle Niedersachen (egal welchen Alters) und alle Hamburger unter 18 in den Dataframe packen.
```{r }
data_filter <- data %>% 
  filter((bundesland == 3 | bundesland == 2) & alter < 18)

nrow(data_filter)
head(data_filter)
```

Ein häufiger Use-Case für Filter, der bisher noch nicht angesprochen wurde, ist es, fehlende Werte aus den Daten herauszufiltern. Das folgende Codebeispiel sortiert Fälle aus, die in der Variable `verbundenheit_europa` einen fehlenden Wert (hier als 99 = "Weiß nicht" codiert) haben:

```{r }
data_filter_na <- data %>% 
  filter(!is.na(verbundenheit_europa))

nrow(data_filter_na)
head(data_filter_na)
```


## Arrange: Fälle sortieren
Mit ´arrange()` lassen sich Fälle in einem Datensatz sortieren. Die Sortierung sollte zwar auf statistische Analysen keinen Einfluss haben, aber dennoch ist dieses Feature nützlich, wenn man z.B. Tabellen hübsch formatieren möchte.

Der Einsatz von `arrange()` ist sehr simpel. Man muss der Funktion nach dem Datensatz lediglich die Variable übergeben nach der sortiert werden soll, hier z.B. nach dem Alter:
```{r }
# aufsteigend sortieren
data %>% 
  arrange(alter) %>% 
  head()
```

Die Daten sind jetzt aufsteigend sortiert. Um eine absteigende Sortierung zu erreichen benötigen wir die Hilfe von `desc()`. Das sieht dann so aus:

```{r }
# absteigend sortieren
data %>% 
  arrange(desc(alter))%>% 
  head()
```


Selbstverständlich kann man auch nach mehreren Variablen sortieren und dabei aufsteigende und absteigende Sortierung nach Belieben mischen:

```{r }
# nach mehreren Variablen sortieren
data %>% 
  arrange(bundesland, alter, desc(geschlecht))%>% 
  head()
```

## Select: Variablen auswählen

Die Funktion `select()` dient genau wie `filter()` dazu, den Datensatz zu verkleinern. Jedoch get es bei `select()` darum, Variablen auszuwählen. Dazu muss man die Variablen, die im Datensatz verbleiben sollen einfach an die Funktion übergeben. Alle anderen Variablen, die nicht vorkommen, werden gelöscht.

```{r }
# Variablen auswählen
data %>% 
  select(alter, geschlecht, verbundenheit_europa) %>% 
  head()
```


Will man nur einzelne Variablen löschen, so geht dies mit einem `-` vor dem Variablennamen `select(data, -alter)` löscht also das Alter, alle anderen Variablen würden aber erhalten bleiben.

Es gibt auch die Möglichkeit, Variablen auszuwählen, die einem bestimmten Schema entsprechen, z.B. deren Name mit "vertaruen_medien_" beginnt. Die Syntax dafür ist `starts_with("vertrauen_medien_")`. Ähnlich kann man auch Variablen in einem bestimmten Bereich auswählen, also alle von `var_name_1` bis `var_name_x`. Dafür müsste man bspw. `alter:bundesland` eingeben.

Zudem kann `select()` auch dazu verwenden, die Variablen im Datensatz umzusortieren. Dazu schreibt man die Variablen einfach in der neuen Reihenfolge in die Funtion. Beim umsortieren gibt es ebenfalls einige nützliche Helfer. Einer ist bspw. die Funktion `everything()` - quasi ein Alias für alle Variablen die bis dahin noch nicht genannt wurden.

```{r }
# Variablen neu sortieren
data %>% 
  select(lfdn, bundesland, everything()) %>% 
  head()
```

## Variablen umcodieren
Eine häufige Aufgabe bei der Datenaufbereitung ist das Umcodieren. Beim Umcodieren wird das Wertespektrum einer Variable verändert oder verdichtet. Ein Anwendungsfall wäre es, stetige Variable damit in Kategorien einteilen (z.B. Altersgruppen bilden). Ein weiterer Anwendungsfall sind Variablen die "falsch herum" codiert wurden zu drehen: In unserem Mini-Datensatz sind beispielsweise die Variablen zu "Verbundenheit" unintutiv codiert: Ein niedriger Zahlenwert entspricht einer hohen Verbundenheit. Der Wert 1 hat das Werte-Label "sehr verbunden", der Wert 5 ist hingegen mit "überhaupt nicht verbunden" codiert. Sie können das im Codebuch sehen, aber das folgende Skript verdeutlicht diesen Umstand an der Variable `verbundenheit_europa`.

```{r, messages=FALSE}
library(sjlabelled)

# einen Vektor mit den Werten einer Variable erzeugen
values = get_values(data$verbundenheit_europa) 
# einen Vektor mit den Labels einer Variable erzeugen
labels = get_labels(data$verbundenheit_europa) 

cbind(values, labels) # beide Vektoren zusammenbinden
```

Intutiver währe es, wenn mit einem hohen Zahlenwert auch eine große Verbundeheit einher ginge. Bei den gepabelten Daten, die hier vorliegen, geht das Umcodieren sehr gut über den Befehl `rec()` aus dem Paket `sjmisc`. Der Befehl ist sehr stark an die Logik von SPSS angelehnt. 

Der `rec()`-Befehl fügt sich in die Tydiverse-Logik ein und erwartet als erstes Argument genau wie die `dplyr`-Funktionen den Dataframe. Deshalb kann man den Befehl ebenfalls sehr gut in der Pipe einsetzen. Das zweite Argeument ist die Variable, die umcodiert werden soll. Man kann hier auch mehrere einsetzen, in unserem Fall alle die mit `verbundenheit_` beginnen. Das letzte und entscheidende Argument ist die Anweisung zur Umcodierung. Es heißt `rec` und beinhaltet einen Text mit den Anweisungen in der Form `"werte_label = neuer_wert"`. Getrennt durch ein Semikolon kann man auch mehrere Anweisungen gleichzeitig übergeben. Jede geplante Umcodierung muss explizit genannt werden. Sollte ein oder mehrere Wert nicht von der Umcodierung betroffen sein, kann man die "restlichen" Werte durch ein `"else=copy"` auffangen. Dadurch wird der Wert aus der urspründlichen Variable einfach in die neue kopiert. In unserem Beispiel betrifft das den Wert 99 = "weiß nicht". Die 99 soll ganz unabhängig von der Umcodierung immer diesen Wert beibehalten.

Die Funktion `rec()` erzeugt neue Variablen, die den gleichen Namen haben wie die ursprünglichen, ergänzt um ein `_r` am Ende. Diese Endung soll deutlich machen, dass es sich um die recodierte Variante der Variablen handelt.

```{r message=FALSE}
library(sjmisc)

data <- data %>% 
  rec(starts_with("verbundenheit_"), rec = "Sehr verbunden = 4;
                                             Ziemlich verbunden = 3;
                                             Nicht sehr verbunden = 2; 
                                             Überhaupt nicht verbunden = 1;
                                             else=copy") 

# Beispielhaft die Variable verbundenheit_europa inkl. recodierter Variante anzeigen:
data %>% 
  select(starts_with("verbundenheit_dtl")) %>% 
  head()
```
Es ist immer ratsam, im Anschluss zu kontrollieren, ob die Umcodierung auch wie erwartet funktioniert hat. Dies kann z.B. über eine Kreuztabelle geschehen oder wie hier durch ein "nebeneinanderlegen" der beiden Variablen.

Eine kleine Ergänzung noch: Selbstverständlich funktioniert `rec()` auch mit nicht-gelabelten Daten. In diesem Fall wären einfach die ursprünglichen Werte statt der (nicht vorhandenen) Wertelabels einzutragen: `"1=4;2=3;3=2;4=1;else=copy"`


## Mutate: Variablen berechnen
Mit `mutate()` kann man neue Variablen bilden. Die Syntax dazu folgt dem Schema `new_var_name = some calculation`. Im nächsten Code-Beispiel wird der Geburtsjahrgang anhand des Alters zum Zeitpunkt der Befragung berechnet. Damit wir die Daten schön vergleichen können, wähle ich die beiden Variablen anschließend aus.

```{r }
# Geburtsjahr berechnen
new_data <- data %>% 
  mutate(jahrgang = 2019 - alter) %>% 
  select(lfdn, alter, jahrgang)

head(new_data)
```

### Variablen unter einer Bedingung berechnen
Man kann Natürlich auch Variablen anhand von logischen Ausdrücken berechnen, also eine Art Filterbedingung dafür zu Rate ziehen welchen Wert die Variable annehmen soll. Es muss dafür wieder mit logischen Ausdrücken gearbeitet werden und wir brauchen eine Funktion die `ìfelse()` heißt. Die Funktion bekommt drei Argumente:

1. Den logischen Ausdruck bei dem für jeden Fall zu prüfen ist, ob er für diesen Fall `TRUE` oder `FALSE` ist.

2. Einen Wert, den die Variable annehmen soll, wenn der Fall `TRUE` eintritt.

3. Einen Wert, den die Variable annehmen soll, wenn der Fall `FALSE` eintritt.

Als Beispiel möchte ich eine Variable berechnen die 1 ist, wenn die Verbundenheit zu Europa größer ist als die zu Deutschland und ansonsten 0. Ich nenne sie `sieht_sich_als_europaeer`.

```{r }
# Variable berechnen mit Bedingung
data_eu <- data %>% 
  mutate(sieht_sich_als_europaeer = ifelse(verbundenheit_europa > verbundenheit_dtl, 1, 0)) 

# Für die kontrolle relevante Variablen auswählen
data_eu %>% 
  select(lfdn, verbundenheit_europa, verbundenheit_dtl, sieht_sich_als_europaeer) %>%   head()
```

<img src="images/ah_dplyr_mutate_blank.png" alt="Artwork by Allison Horst" style="width:1000px;margin-bottom:5px;margin-top:50px">
<div style="color:grey;text-align:right">Artwork by <a href="https://github.com/allisonhorst/stats-illustrations/">Allison Horst</a></div><br />

## Summary: Daten verdichten

Die letzte `dplyr`-Funktion auf die ich hier eingehen möchte ist `summarize()`. Im ersten Moment wirkt `summarize()` vielleicht ein bisschen wie eine komplizierte Art deskriptiven Statistiken zu berechnen. Die Funktion kann aber viel mehr und das Entscheidende ist, dass sie nicht wie die im Kapitel "Deskriptive Statistiken vorgestellten Funktionen einfach nur einen Kennwert zurückgibt, sondern einen Datensatz mit dem Ergebnis. 

Möglicherweise werden Sie die Funktion zunächst kaum benutzen, aber später wiederentdecken. Der Vollständigkeit halber wird sie trotzdem an dieser Stelle kurz erläutert.

Im ersten Beispiel möchte ich den Mittelwert für die im vorigen Abschnitt berechnete Variable `sieht_sich_als_europaeer` ausrechnen. 

```{r }
# Test der summarize-Funktion
data_eu %>% 
  summarize(mean = mean(sieht_sich_als_europaeer, na.rm = TRUE))
```

Das Ergebnis ist ein Datensatz, der eine neue Variable enthält die `mean` heißt und nur einen Fall hat. Soweit, so unspannend. Der Wert ist `r mean(data_eu$sieht_sich_als_europaeer)` und darf als Anteilswert interpretiert werden, da die Variable 0/1-codiert ist. Knapp 32 Prozent der Befragten fühlen sich also stärker mit Europa als mit Deutschland verbunden.

Das geschickte an `summarize()` ist, dass die Funktion perfekt mit `group_by()` zusammenarbeitet. Mit `group_by()` kann man einen Dataframe aufteilen, so dass er dann wie mehrere getrennte Datensätze behandelt wird. Wir könnten also Gruppen bilden und die Anteile in diesen Gruppen rein deskriptiv vergleichen. Mich interessiert bspw. ob regionale Unterschiede gibt. Vergleichen wir mal Hamburg (Wert 2) und Niedersachsen (Wert 3).


```{r }
# summarize mit filter & group_by
data_eu %>%
  filter(bundesland == 2 | bundesland == 3) %>% 
  group_by(bundesland) %>% 
  summarize(mean = mean(sieht_sich_als_europaeer, na.rm = TRUE))
```

Natürlich funktioniert das nicht nur mit dem arithmetischen Mittel. Auch andere Berechnungen wären hier denkbar.
